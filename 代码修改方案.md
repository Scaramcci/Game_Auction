# 代码修改

下面的建议分两块：



- **A | 让 λt-βt 自动随信息递减 / 递增**（修掉“深度水平线”问题）；
- **B | 把“真实价值 v” 设成 多段 Piece-wise 信号**——例如 10 轮为一段，然后抽一条新 v，再跑 10 轮——以观察多段信息的吸收轨迹。





我直接给出 **env.py 核心改动**（★处是新增/修改），其余脚本只需少量调用接口即可兼容；不再一行行贴满，只保留最关键 patch，便于你对照粘贴。



------





## **A | 动态 λ**

## **t**

##  **与 β**

## **t**



```
# env.py 关键段 ----------------------------------------------------
class InsiderKyleEnv(gym.Env):
    def __init__(self,
                 T=10,
                 sigma_u=1.0,
                 sigma_v=1.0,
                 max_action=5.0,
                 dynamic_lambda=True,          # ★ 新增开关：True = 按理论递减
                 seed=None,
                 super_horizon=1):             # ★ 下节会用到
        ...
        self.dynamic_lambda = dynamic_lambda
        self.super_horizon  = super_horizon    # 外层 epoch ⟹ 一段信息持续多少 inner-epochs
        self.reset()

    # ---------- 理论 β_t 公式 ----------
    def _theory_beta(self):
        remain = self.T - self.t + 1           # 还剩多少 inner-step
        return (self.sigma_u / np.sqrt(self.cur_var)) * np.sqrt(remain / (remain + 1))

    # ---------- reset ----------
    def _reset_inner_state(self):
        self.v        = self.np_random.normal(0., self.sigma_v)
        self.cur_price= 0.0
        self.cur_var  = self.sigma_v ** 2
        self.t        = 0                      # inner-step 计数
        self.beta_hist, self.lambda_hist = [],[]

    def reset(self):
        self.outer_cnt = getattr(self, 'outer_cnt', -1) + 1   # 外层 epoch 计数
        self._reset_inner_state()
        return np.array([0., self.cur_price, self.v], dtype=np.float32)

    # ---------- step ----------
    def step(self, action):
        self.t += 1
        # 1) insider order
        x = float(action[0])

        # 2) noise order
        u = self.np_random.normal(0., self.sigma_u)
        Q = x + u

        # 3) ★ 动态 λ_t (或常数)
        if self.dynamic_lambda:
            beta_star     = self._theory_beta()
            self.lambda_t = 1.0 / (2.0 * beta_star)
        else:
            self.lambda_t = getattr(self, 'lambda_t', 0.4)    # 固定
            beta_star     = self._theory_beta()               # 仍可算理论对比
        self.beta_hist.append(beta_star)
        self.lambda_hist.append(self.lambda_t)

        # 4) 更新价格
        prev_p     = self.cur_price
        self.cur_price = prev_p + self.lambda_t * Q

        # 5) 更新方差（精确）
        self.cur_var = (self.cur_var * self.sigma_u**2) / (beta_star**2 * self.cur_var + self.sigma_u**2)

        # 6) reward
        reward = x * (self.v - self.cur_price)

        # 7) 观测 & done
        done_inner  = (self.t >= self.T)
        done_outer  = done_inner and ((self.outer_cnt + 1) % self.super_horizon == 0)
        # ★ 若仅结束 inner-epoch：换新 v 保持同一 episode；若 super-horizon 到期才真正 done
        if done_inner and not done_outer:
            # 只换 v，不清 PPO 隐藏状态 → 观察 agent 适应新信息
            self._reset_inner_state()
        obs = np.array([self.t / float(self.T), self.cur_price, self.v], dtype=np.float32)
        return obs, reward, done_outer, {"noise": u, "Var": self.cur_var}
```

**解释**



- dynamic_lambda=True 时，每轮用理论公式 λt=1/(2βt) 自动递减；
- super_horizon = 外层 epoch 长度。若设 super_horizon=5、T=10，则一次完整 episode 共 50 inner-steps；每满 10 inner步换一个新 v；PPO 能在**连续 episode 内见到多段信号**。
- done_outer 只在跑完 super_horizon 段后返回 True，让 SB3 截断轨迹；否则内部换信号但继续采样，能学**信息到期切换**的策略。





------





## **B | 多段信息：如何使用**



```
# 训练脚本
env = InsiderKyleEnv(T=10,
                     sigma_u=1.0,
                     dynamic_lambda=True,
                     super_horizon=3,     # 每 3×T = 30 inner-steps 换一次 episode
                     seed=42)

# 观察信息刷新：
obs = env.reset()
done=False
while not done:
    action = env.action_space.sample()   # or model.predict
    obs, rew, done, info = env.step(action)
    if env.t == 1:                       # inner-reset 后 t 从 1 重新开始
        print(f'-->> switched to new v={env.v:.3f}  outer={env.outer_cnt}')
```

*可视化*：在 visualize.py 里若要区分各段信号，只要在价格图加竖直分割线：

```
for k in range(1, env.super_horizon):
    plt.axvline(k*env.T, color='grey', ls='--', alpha=0.3)
```



------





## **C | 训练超参微调**





1. **更长轨迹** ⇒ n_steps 设为 T*super_horizon 的整数倍，可以让 PPO 一批样本覆盖完整 outer-epoch。
2. **奖励缩放** ⇒ 如果换 v 导致 reward 分布漂移，给 PPO 加 normalize_advantage 或手动除以 sigma_v.
3. **lambda / beta 监控** ⇒ 建议把 env.lambda_hist、env.beta_hist 塞进 info，在自写 callback 里收集 → TensorBoard scalars。





------





## **D | 常见掉坑提醒**



| **现象**     | **修复**                                                     |
| ------------ | ------------------------------------------------------------ |
| 深度仍水平   | 确认 dynamic_lambda=True、inner var cur_var 真在递减（打印） |
| 价格收敛变慢 | 多段 v 使 agent 频繁换目标 → 训练迭代数要 ↑；或给 agent 提供“剩余 super-step” 作为状态 |
| λt 抖动      | 你的 action-noise 大；在评价图里用 *回归 λ̂* 而非单轮 λt 做平滑 |



------





### **小结**





- **先把 λt 从常数改成理论动态函数**，深度曲线就会向论文预期（递增）迈进；
- **再用 super_horizon 封装外层 epoch**，即可在一次 episode 内连续注入 *多段真实价值*，考察市场如何连续地“遗忘旧信息 / 消化新信息”。





这样既能 reproduces 经典 Kyle 结果，也给多段信号场景留下扩展钩子。

如需更详细 diff 或 callback 示例，随时再 ping!